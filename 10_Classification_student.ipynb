{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reminders about Supervised, Unsupervised and Reinforcement Learning\n",
    "\n",
    "One of the ways of differentiating categories of machine learning algorithms is to split them into **supervised**, **unsupervised** and **reinforcement** learning methods. \n",
    "\n",
    "With **supervised** methods, the training data come with the outcome you are trying to predict--they are labeled. For example, with linear regression, we have linked input and output values and are predicting the function that relates them. With image classification, we might start with a set of images that are labeled as cats, dogs, people, etc., and then train the algorithm to learn to classify a new image into these classes.\n",
    "\n",
    "With **unsupervised** methods, we have unlabeled data (no y-values or pre-defined categories) and ask the algorithm to learn patterns from the data itself. These methods usually try to categorize the observations.\n",
    "\n",
    "With **reinforcement learning**, the training data is also unlabeled, but the system received feedback for its actions/categorizations. Game systems are a classic example of this--learn to play chess by playing games and learning what moves lead to a winning strategy. Robotics and autonomous vehicles are other applications--learn how to climb stairs by getting points for actions that leads to going up stairs.\n",
    "\n",
    "There are also methods that mix these.\n",
    "\n",
    "## Types of Supervised Learning\n",
    "\n",
    "The two main types of supervised learning are **classification** and **regression**.\n",
    "\n",
    "In **classification**, the response variable (y) is categorical or discrete. For example, this image is a cat, dog, bird, etc.; you do or do not have some disease; this plant is a desired crop or a weed to kill; etc.\n",
    "\n",
    "In **regression**, the response variable (y) is continuous or numerical. in the automobile fuel efficiency data we've been working with, miles per gallon is the response variable and the regression approaches we've been looking at are finding the model that takes the input (hp) and predicts the output (mpg).\n",
    "\n",
    "The image below from the [maplearn.ml package documentation](https://maplearn.readthedocs.io/en/latest/maplearn.ml.html) provides a nice visual of the general idea of classification and regression.\n",
    "\n",
    "![Classification vs regression. Image from maplearn.ml documentation](images/classif_reg.maplearn.png)\n",
    "\n",
    "\n",
    "## Types of classification\n",
    "\n",
    "Many methods have been developed for classification and the list below is certainly not comprehensive. Nor can we hope to cover all of them in this class. But hopefully this list gives you a sense of the diversity and places to start looking.\n",
    "\n",
    "* Linear Classifiers\n",
    "  * Logistic regression\n",
    "  * Naive Bayes classifier\n",
    "  * Fisher‚Äôs linear discriminant\n",
    "* Kernel methods\n",
    "  * Support vector machines\n",
    "  * Least squares support vector machines\n",
    "* k-nearest neighbors\n",
    "* Decision trees\n",
    "  * Random forests\n",
    "* Neural networks\n",
    "* Learning vector quantization\n",
    "\n",
    "\n",
    "We will look at a few of these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Despite its name, **logistic regression** is a **classification** algorithm. Logistic regression estimates the probability of an observation belonging to each class, given the values of the predictor variables (features). This is then used, in conjunction with a threshold to predict discrete (most often binary) class association--1/0, yes/no, true/false--based on input data. Logistic regression predicts the probability of occurrence of an event by fitting a **logit function**. \n",
    "\n",
    "A common example of logistic regression is the task of classifying email as spam or not-spam. I found this [Introduction to Logistic Regression](https://courses.lumenlearning.com/introstats1/chapter/introduction-to-logistic-regression/) particularly helpful in preparing this.\n",
    "\n",
    "Logistic regression is a generalized linear model where the outcome is a binary (two-levels) categorical variable. What we are looking for is actually **probabilities**: What is the probability of each possible outcome? Outcome 1 (spam) happens with the probability of $p_i$ and outcome 0 (not-spam) with the probability $1-p_i$.\n",
    "\n",
    "The **odds** of an event are the ratio of the probability that the event occurs to the probability that it doesn't. \n",
    "\n",
    "$$ odds\\,of\\,event = \\frac{p}{1-p} $$\n",
    "\n",
    "Remember our standard linear regression equation:\n",
    "\n",
    "$$ \\hat{y} = \\beta_0  + \\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i$$\n",
    "\n",
    "In this, $\\hat{y}$ can have any range. To get a probability, we need to transform $\\hat{y}$ such that it has values between 0 and 1. A common transformation is the **logit transformation**.\n",
    "\n",
    "$$ logit(y) = ln(odds) = ln(\\frac{p_i}{1-p_i}) $$\n",
    "\n",
    "So we end up with:\n",
    "\n",
    "$$  ln(\\frac{p_i}{1-p_i}) = \\beta_0  + \\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i$$  \n",
    "\n",
    "To solve for $p$, we can take the antilog and do a bunch of algebra to end up with:\n",
    "\n",
    "$$ p = \\frac{1}{1+e^{-(\\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i)}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression and the Sigmoid and Natural Log Functions\n",
    "\n",
    "Logistic regression makes use of two key functions that we'll review here.\n",
    "\n",
    "First, the **sigmoid function** pictured below. The sigmoid function has values very close to (but not equal to) 0 or 1 for most of its distribution, making it helpful for classification.\n",
    "\n",
    "$$ \n",
    "    \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{1}{1+exp^{-x}}\n",
    "$$\n",
    "And think of $\\sigma(x)$ as *y*.\n",
    "\n",
    "#### The Sigmoid function $\\frac{1}{1+exp^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code to generate sigmoid graph below\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(-10,10, num=100) # use randome numbers from a range of -10 to 10\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show\n",
    "\n",
    "#transforms a numerical probability based on categroial values. Transformaiton fo continues variable into another variable that acts as a proability y is the probabiltiy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Natural log function: $log(x)$  in green, $log(1-x)$ in blue\n",
    "\n",
    "The natural log function is shown below. Note that in Python, `math.log(x)` and `numpy.log(x)` represent the natural logarithm of `x`, so we'll use **log** vs **ln**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.001,0.999, num=100) # need to update code\n",
    "y = np.log(x)\n",
    "\n",
    "\n",
    "label=\"log(x)\"\n",
    "df = pd.DataFrame(x,y)\n",
    "\n",
    "pn.ggplot(df, pn.aes(x='x', y='y')) + pn.geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Mirko Stojiljkoviƒá's Logistic Regression in Python](https://realpython.com/logistic-regression-python/)\n",
    "\n",
    "> Your goal is to find the **logistic regression function $p(x)$** such that the **predicted responses $p(x_i)$** are as close as possible to the **actual response $y_i$** for each observation $i=1,\\ldots,n$. Remember that the actual response can be only 0 or 1 in binary classification problems! This means that each $p(x_i)$ should be close to either 0 or 1. That‚Äôs why it‚Äôs convenient to use the sigmoid function.\n",
    "\n",
    "### Logistic Regression Methodology\n",
    "\n",
    "Again, from From [Mirko Stojiljkoviƒá's Logistic Regression in Python](https://realpython.com/logistic-regression-python/):\n",
    "\n",
    "> Logistic regression is a linear classifier, so you‚Äôll use a linear function $f(x) = b_0 +b_1x_1 + \\ldots+b_tx_t$, also called the **logit**. The variables $b_0, b_1, \\ldots, b_t$ are the estimators of the regression coefficients, which are also called the **predicted weights** or just **coefficients**.\n",
    "\n",
    "Sound familir??\n",
    "\n",
    "> The logistic regression function $p(x)$ is the sigmoid function of $f(x)$: \n",
    ">\n",
    "> $$ p(x) = \\frac{1}{1+exp^{-f(x)}} $$\n",
    ">\n",
    "> As such, it‚Äôs often close to either 0 or 1. The function  $p(x)$ is often interpreted as the predicted probability that the output for a given $x$ is equal to 1. Therefore, $1-p(x)$ is the probability that the output is 0.\n",
    ">\n",
    "> Logistic regression determines the best predicted weights $b_0, b_1, \\ldots, b_t$ such that the function $p(x)$ is as close as possible to all actual responses $y_i, i=1,\\ldots,n$ where $n$ is the number of observations. The process of calculating the best weights using available observations is called **model training** or **fitting**.\n",
    ">\n",
    ">To get the best weights, you usually maximize the **log-likelihood function (LLF)** for all observations  $i=1,\\ldots,n$. This method is called the **maximum likelihood estimation** and is represented by the equation \n",
    ">\n",
    "> $$ LLF = \\sum_{i}^{n} (y_ilog(p(x_i)) + (1-y_i)log(1-p(x_i)) $$\n",
    ">\n",
    "> When $y_i=0$, the LLF for the corresponding observation is equal to $log(1 - p(x_i)))$. If $p(x_i)$ is close to $y_i=0$, then $log(1 - p(x_i)))$ is close to 0. This is the result you want. \n",
    ">\n",
    "> If $p(x_i)$ is far from 0, then $log(1 - p(x_i))$ drops significantly. You don‚Äôt want that result because your goal is to obtain the maximum LLF. \n",
    ">\n",
    "> Similarly, when $y_i=1$, the LLF for that observation is $y_ilog(p_x)$. If $p(x_i)$ is close to $y_i=1$, then $log(p(x_i))$ is close to 0. If $p(x_i)$ is far from 1, then $log(p(x_i))$ is a large negative number.\n",
    ">\n",
    "> There‚Äôs one more important relationship between $p(x)$ and $f(x)$ which is that:\n",
    ">\n",
    ">$$ f(x) =log \\frac{(p_x)}{(1-p(x))} $$\n",
    ">\n",
    "> This equality explains why $f(x)$ is the [logit](https://en.wikipedia.org/wiki/Logit). It implies that $p(x)=0.5$ when $f(x)=0$ and that the predicted output is 1 if $f(x) > 0$ and 0 otherwise.\n",
    "\n",
    "### Single Variable Logistic Regression example\n",
    "\n",
    "Let's try to illustrate this with a small example using a single variable. If it helps to make it concrete, lets imagine that we are classifying movies based on the number of actors I know. I only want to watch movies with 4 or more actors I know...\n",
    "\n",
    "Actors I know | Watch the movie?\n",
    "--------------|-----------------\n",
    "0  | No\n",
    "üßë | No\n",
    "üßëüë©üèø‚ÄçüöÄ | No\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠ | No\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠üë©üèº‚Äçüíº|Yes\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠üë©üë≥üèª‚Äç‚ôÇ|Yes\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠üë©üë≥üßî|Yes\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠üë©üë≥üßîüë©üèæ‚Äçü¶∞|Yes\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠üë©üë≥üßîüë©üèæ‚Äçü¶∞üßïüèΩ|Yes\n",
    "üßëüë©üèø‚ÄçüöÄüë©üèæ‚Äçüè≠üë©üë≥üßîüë©üèæ‚Äçü¶∞üßïüèΩü§πüèº‚Äç‚ôÄÔ∏è|Yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making mock data to represent wanting to watch movies if there are 4 or more known actors\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's make some data to work with\n",
    "   # Reshape to column vector\n",
    "x = np.arange(10).reshape(-1,1)\n",
    "y = np.array([0,0,0,0,1,1,1,1,1,1]) #creating an array in a list. Values are listed from psotion 0 1 2, 3  then value moving forwrd\n",
    "\n",
    "y\n",
    "# Question what are these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of options for the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier. For this example, we will use the `liblinear` solver--the algorithm used in the optimization step. This is mostly for two reasons...\n",
    "1. Until recently this was the default (*lbfgs* is now default--[here is](https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451) a good article on the options) \n",
    "1. It produces some \"interesting\" results to discuss. Certainly look at the options and play with different solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('logistic', LogisticRegression(solver='liblinear'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit a model using our data\n",
    "model = Pipeline([\n",
    "    ('logistic', LogisticRegression(solver = 'liblinear'))])\n",
    "model.fit(x,y) #fitting the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1]\n",
      "Intercept: [-1.04608067]\n",
      "Coefficients: [[0.51491375]]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the model parameters\n",
    "\n",
    "print(\"Classes:\", model['logistic'].classes_)\n",
    "print(\"Intercept:\", model['logistic'].intercept_)\n",
    "print(\"Coefficients:\", model['logistic'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0315648542136064\n"
     ]
    }
   ],
   "source": [
    "# Get predicted y's for f(x)\n",
    "x_plot= np.linspace(0,10, num=100)\n",
    "y_fx = model['logistic'].intercept_[0] + ( model['logistic'].coef_[0][0] * x_plot )\n",
    "\n",
    "# Find where f(x)=0 so that we can plot that line\n",
    "fx_0 = -1 * model['logistic'].intercept_[0] / model['logistic'].coef_[0][0] \n",
    "\n",
    "# Use the logistic to get y's for p(x)\n",
    "y_px = 1 / (1 + np.exp(-1 * y_fx) )\n",
    "\n",
    "\n",
    "# Sorry, after fighting with plotnine for a bit I fell back to matplotlib for this plot\n",
    "plt.ylim([-0.1,1.1]) # Set Y-limits\n",
    "plt.grid(True)\n",
    "plt.scatter(range(10),y, color='green') # Plot the true y's in green\n",
    "plt.plot(x_plot,y_fx, color='black', linestyle='dashed') # Plot f(x) line as black, dashed\n",
    "plt.axvline(fx_0) # Plot line where f(x)=0, implying p(x)=0.5 Points to the left 0, right 1\n",
    "plt.plot(x_plot, y_px, color='grey') # Plot the p(x) line as grey, solid\n",
    "plt.show()\n",
    "print(fx_0)\n",
    "\n",
    "#model represents a model of knowing 3 or less acutors a value of 0 and a value of knwoing 4 vlaue a value of 1. There is an update an issue with rhe vlaue of 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74002157, 0.25997843],\n",
       "       [0.62975524, 0.37024476],\n",
       "       [0.5040632 , 0.4959368 ],\n",
       "       [0.37785549, 0.62214451],\n",
       "       [0.26628093, 0.73371907],\n",
       "       [0.17821501, 0.82178499],\n",
       "       [0.11472079, 0.88527921],\n",
       "       [0.07186982, 0.92813018],\n",
       "       [0.04422513, 0.95577487],\n",
       "       [0.02690569, 0.97309431]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How does the model do?\n",
    "model.predict_proba(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output matrix above, each row corresponds to a single observation ($x$). The first column is the predicted probability of the output being the  $0^{th}$ class-- 0 or $(1-p(x))$. The second column is the predicted probability of class 1, $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x) #model predicting the data points. The percentage of the model correctly classifying points # numebr 3 is now being cateogorized as wathcing a movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above example and image below come from the Real Python blog [*Logistic Regression in Python*](https://realpython.com/logistic-regression-python/).\n",
    "\n",
    "![](images/log-reg-5_real_python.png)\n",
    "\n",
    "AS in the graph we made, the green circles represent the actual responses ($y$) that are correctly predicted by the model. The red x shows the incorrect prediction for point 4. The solid line is the estimated logistic function $p(x)$, and the grey squares along that line are the predicted responses (second column in table). The black dashed line is the logit, $f(x)$.\n",
    "\n",
    "As the plot above and the `model.predict(x)` output show, the model correctly classifies nine of the 10 observations. As such, the accuracy is 9/10=0.9, which is available with the `model.score()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the model score\n",
    "model.score (x,y) # used to get the percentage at times when the model is predicting the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics\n",
    "\n",
    "The `model.score()` tells us something about our model's performance, but not really everything we would like to know. \n",
    "\n",
    "Taking a different classification problem, let's look at a model that classifies images as either cat or not cat. Here are common terms used to describe classification success:\n",
    "\n",
    "![Cat classification success](images/classification_success.png)\n",
    "\n",
    "One way of gaining insight into our model is what is called a **confusion matrix**. This summarizes the true and false positive and negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [0 6]]\n"
     ]
    }
   ],
   "source": [
    "#Show the confusion matrix\n",
    "cm = confusion_matrix(y, model.predict(x))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'logisic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/23714479/ipykernel_2693374/1996216603.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcm_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logisic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/apps/python/3.8/lib/python3.8/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m# Not an int, try get step by name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'logisic'"
     ]
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "cm_display = ConfusionMatrixDisplay(cm, display_labels = model['logisic'].classes_).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What kind of error is our model making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Classification Metrics\n",
    "\n",
    "For the following, we will refer to the 0 class as negative and the 1 class as positive. \n",
    "\n",
    "**Precision**: The ability of the classifier not to label as positive (1) a sample that is negative (0). Out of those predicted positive, how many are actual positive.\n",
    "\n",
    "$$ Precision =  \\frac{True\\,Positive}{True\\,Positive\\,+\\,False\\,Positive} =  \\frac{True\\,Positive}{Total\\,Predicted\\,Positive} $$\n",
    "\n",
    "Precision is a good measure when the cost of False Positive is high. E.g. Email spam filter. If lots of email is falsely flagged as spam and discarded, you might miss important emails. Precision is also called the *positive predictive value*. There is also a *negative predictive value* that is the ratio of true negatives over the sum of true and false negatives.\n",
    "\n",
    "**Recall/Sensitivity**:  The ability of the classifier to find all the positive (1) samples. How many of the actually positives does the model label as positive.\n",
    "\n",
    "$$ Recall = \\frac{True\\,Positive}{True\\,Positive + False\\,Negative} =  \\frac{True\\,Positive}{Total\\,Actual\\,Positive} $$\n",
    "\n",
    "Recall is a good measure when the cost of False Negative is high. E.g. disease diagnosis. If a sick patient is predicted as not sick, the cost associated with a false negative is high. Recall is also called *sensitivity* or *True Positive Rate* (**TPR**).\n",
    "\n",
    "**Specificity**: The ability of the classifier to fine all the negative (0) samples. How many of the actually negatives does the model label as negative.\n",
    "\n",
    "$$ Specificity = \\frac{True\\,Negative}{True\\,Negative + False\\,Positive} = \\frac{True\\,Negative}{Total\\,Actual\\,Negative} $$\n",
    "\n",
    "Since we'll use this later, 1 - Specificity is known as the *False Positivity Rate (**FPR**)*.\n",
    "\n",
    "**F1-Score** also known as F-beta: A weighted harmonic mean of the precision and recall, where and F-beta score reaches its best value at 1 and worst score at 0. \n",
    "\n",
    "\n",
    "The F-beta score weighs recall more than precision by a factor of `beta`. `beta=1.0` means recall and precision are equally important. \n",
    "\n",
    "\n",
    "$$ F1 = 2\\frac{Precision\\,*\\,Recall}{Precision\\,+\\,Recall} $$\n",
    "\n",
    "**Support**: The number of samples in each class.\n",
    "  \n",
    "**Accuracy**: Total number of correct predictions over all predictions. Only accurate with balanced models (relatively equal frequency of 0/1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classfication_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/local/23714479/ipykernel_2693374/1947570214.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Another set of classification metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassfication_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'classfication_report' is not defined"
     ]
    }
   ],
   "source": [
    "# Another set of classification metrics\n",
    "print(classfication_report(y, model.predict(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression example\n",
    "\n",
    "For this example, let's predict if a patient will develop (1) diabetes or not (0). Note that this is a different dataset than we have been using. We have several variables that can be used for this prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/python/3.8/lib/python3.8/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "..                        ...  ...      ...  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as pn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "diabetes = pd.read_csv(\"data/diabetes.csv\")\n",
    "\n",
    "\n",
    "diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup train/test data\n",
    "y = diabetes.Outcome\n",
    "X = diabetes.drop(columns='Outcome') # drop the columsn that are labeled outcoem int eh matrix due to keeping it in the y value\n",
    "# Get the y values and drop from df\n",
    "\n",
    "# Split to 75% train, 25% test\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y , test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('logistic', LogisticRegression(solver='liblinear'))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit a model using our data\n",
    "model = Pipeline([\n",
    "    ('logistic', LogisticRegression(solver='liblinear'))\n",
    "     ])     \n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score on training data: 0.7569444444444444\n",
      "Model score on testing data: 0.8072916666666666\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "# Predict ys using model and test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Get model.score for both training and test data\n",
    "print(f'Model score on training data: {model.score(X_train,y_train)}')\n",
    "print(f'Model score on testing data: {model.score(X_test,y_test)}')\n",
    "\n",
    "# 80% accuray on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,  11],\n",
       "       [ 26,  36]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the confusion matrix\n",
    "\n",
    "confusion_matrix(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x14d7ff32a190>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred), display_labels=model['logistic'].classes_).plot() # looking at model of confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.87       130\n",
      "           1       0.77      0.58      0.66        62\n",
      "\n",
      "    accuracy                           0.81       192\n",
      "   macro avg       0.79      0.75      0.76       192\n",
      "weighted avg       0.80      0.81      0.80       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log odds\n",
    "\n",
    "One nice feature of logistic regression is that the coefficients are log odds or effect. We can get the $exp(coef\\_)$ and get the odds. In this example, for every unit increase in DiabetesPedigreeFunction, the odds of diabetes goes up by 1.8, or nearly doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# odds = exp(coef_)\n",
    "\n",
    "\n",
    "# Create a dataframe and sort by odds column with highest at top\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "Originating [rating radar operators in the 1940s](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#History), a ROC curve is a way to visualize the tradeoff between sensitivity and specificity.\n",
    "\n",
    "So far, we have been assuming anything below 0.5 is class 0 and anything about 0.5 is class 1. But that is one of any number of cutoffs and we could change the classification threshold.\n",
    "\n",
    "**What effect would having a lower threshold have on false positive and negative rates?**\n",
    "\n",
    "**When might you want to have more false positives? or more false positives?**\n",
    "\n",
    "Remember:\n",
    "* Recall = Sensitivity = TPR = true positives / total actual positives\n",
    "* Specificity = true negatives / total actual negatives\n",
    "  * 1 - Specificity = FPR\n",
    "\n",
    "The ROC curve plots these against each other at classification thresholds from 0 to 1. Often a line for a random model (randomly classify observations as 0/1--should be 50% accurate) is also added.\n",
    "\n",
    "The **Area Under the Curve (AUC)** is an aggregate measure of model performance across all classification thresholds. \n",
    "\n",
    "The higher the AUC, the better the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresh = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC w/ RocCurveDisplay\n",
    "#display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='example estimator')\n",
    "#display.plot() \n",
    "\n",
    "# Plot ROC w/ plotnine\n",
    "df= pd.DataFrame(fpr,tpr)\n",
    "(pn.ggplot(df, pn.aes(x='fpr', y='tpr')) + pn.geom_line() +\n",
    " pn.geom_point(pn.aes(color=thresh)) +\n",
    " pn.scale_color_gradient(low='yellow', high='red', name='Threshold', limits=[0,1]) + \n",
    " pn.geom_text(pn.aes(x= 0.5, y= 0.1, label=auc)) + \n",
    " pn.geom_abline(intercept=0, slope=1, color='blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
